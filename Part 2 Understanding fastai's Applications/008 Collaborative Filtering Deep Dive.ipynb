{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xNmKJZP3YPkL"
   },
   "source": [
    "*Collaborative Filtering:* look at which products the current user has used or liked, find other users who have used or liked similar products, and then recommend other products that those users have used or liked.\n",
    "\n",
    "*latent factors:* there some underlying concept of sci-fi, action, and movie age, and these concepts must be relevant for at least some people's movie-watching decisions.\n",
    "\n",
    "Learn:\n",
    "- Movie recommendation problem\n",
    "- start by getting some data suitable for a collaborative filtering model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-6Oaiq0EkXBV"
   },
   "source": [
    "## A First Look at the Data\n",
    "MovieLens: dataset of movie watching history, tens of millions of movie rankings:\n",
    "- a movie ID\n",
    "- a user ID\n",
    "- a numeric rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 37
    },
    "id": "TItBVRNPliCH",
    "outputId": "a2d31cd3-d17b-47b0-d9c9-6112da67fed3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='4931584' class='' max='4924029' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.15% [4931584/4924029 00:00<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# use a subset of 100k of them:\n",
    "\n",
    "from fastai.collab import *\n",
    "from fastai.tabular.all import *\n",
    "path = untar_data(URLs.ML_100k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DdqcSdeKmWyV"
   },
   "source": [
    "The main table is in the file u.data. It is tab-separated and the columns are:\n",
    "- user\n",
    "- movie\n",
    "- rating\n",
    "- timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "t1bNuznzmDGS",
    "outputId": "ff0721e3-97cc-49d7-d260-f44c8243254f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>movie</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>196</td>\n",
       "      <td>242</td>\n",
       "      <td>3</td>\n",
       "      <td>881250949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>186</td>\n",
       "      <td>302</td>\n",
       "      <td>3</td>\n",
       "      <td>891717742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22</td>\n",
       "      <td>377</td>\n",
       "      <td>1</td>\n",
       "      <td>878887116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>244</td>\n",
       "      <td>51</td>\n",
       "      <td>2</td>\n",
       "      <td>880606923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>166</td>\n",
       "      <td>346</td>\n",
       "      <td>1</td>\n",
       "      <td>886397596</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user  movie  rating  timestamp\n",
       "0   196    242       3  881250949\n",
       "1   186    302       3  891717742\n",
       "2    22    377       1  878887116\n",
       "3   244     51       2  880606923\n",
       "4   166    346       1  886397596"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# indicate names when reading the file with Pandas\n",
    "# open and take a look:\n",
    "\n",
    "ratings = pd.read_csv(path/'u.data', delimiter='\\t', header=None,\n",
    "                      names=['user', 'movie', 'rating', 'timestamp'])\n",
    "\n",
    "ratings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KE8anhhM5HDx"
   },
   "source": [
    "The empty cells(user has not reviewed the movie yet) in crosstab are the things that model to learn to fill in(figure out which of those movies they mignt be most likely to enjoy)\n",
    "\n",
    "Assuming factors range between -1(indicating weaker matches) and +1(indicating stronger matches), and the categories are:\n",
    "- [sci-fi, action, old movies]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "MerfAs3KnN6b"
   },
   "outputs": [],
   "source": [
    "# represent the movie The Last Skywalker as follows:\n",
    "\n",
    "last_skywalker = np.array([0.98, 0.9, -0.9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "V8cvzawpE6N6"
   },
   "outputs": [],
   "source": [
    "# represent a user who likes modern sci-fi action movies as follows:\n",
    "\n",
    "user1 = np.array([0.9, 0.8, -0.6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E9PRSPPXFM2c",
    "outputId": "4f20579b-858c-4e23-eff8-58f34231c5b0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.1420000000000003"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate the match between this combinations:\n",
    "\n",
    "(user1*last_skywalker).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jkgiIMkqFins"
   },
   "source": [
    "Multiply two vectors together and add up the results: Dot product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "41SZ38bIFUMm"
   },
   "outputs": [],
   "source": [
    "# represent the movie Casablanca as follows:\n",
    "\n",
    "casablanca = np.array([-0.99, -0.3, 0.8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2e6W8s2RGC2a",
    "outputId": "426171c1-523f-49d6-ee44-626f276b4c94"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.611"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# match between this combination is shown here:\n",
    "\n",
    "(user1*casablanca).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lxgj8ZZoJXKD"
   },
   "source": [
    "## Learning the Latent Factors\n",
    "STEP1: Randomly initialize some parameters(set of latent factors for each user and movie)\n",
    "\n",
    "STEP2: Calculate our predictions. By taking dot product of each movie with each user.\n",
    "- high product: if user likes action movies and movie has lots of action or vice-versa.\n",
    "- low product: if we have a mismatch.\n",
    "\n",
    "STEP3: Calculate our loss. Pick MSE, resonable way to represent the accuracy of a prediction.\n",
    "\n",
    "Optimize our parameters(the latent factors) using stochastic gradient descent:\n",
    "- minimize the loss\n",
    "- calculate the match using dot product\n",
    "- campare it to actual rating\n",
    "- then calculate the derivative of this value\n",
    "- step the weights by multiplying this by the LR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fmc3k47LrChY"
   },
   "source": [
    "## Creating the DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "XezX2Y0xGJrB",
    "outputId": "0aa12bc7-3dd6-4949-94eb-48f5a7532bae"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>GoldenEye (1995)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Four Rooms (1995)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Get Shorty (1995)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Copycat (1995)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   movie              title\n",
       "0      1   Toy Story (1995)\n",
       "1      2   GoldenEye (1995)\n",
       "2      3  Four Rooms (1995)\n",
       "3      4  Get Shorty (1995)\n",
       "4      5     Copycat (1995)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The table u.item contains the correspondence of IDs to titles:\n",
    "movies = pd.read_csv(path/'u.item', delimiter='|', encoding='latin-1',\n",
    "                     usecols=(0,1), names=('movie','title'), header=None)\n",
    "\n",
    "movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "8Tyigz2FsCkN",
    "outputId": "3df9e1fe-8754-4b5d-cedf-d3752f701c4c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>movie</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>196</td>\n",
       "      <td>242</td>\n",
       "      <td>3</td>\n",
       "      <td>881250949</td>\n",
       "      <td>Kolya (1996)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>63</td>\n",
       "      <td>242</td>\n",
       "      <td>3</td>\n",
       "      <td>875747190</td>\n",
       "      <td>Kolya (1996)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>226</td>\n",
       "      <td>242</td>\n",
       "      <td>5</td>\n",
       "      <td>883888671</td>\n",
       "      <td>Kolya (1996)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>154</td>\n",
       "      <td>242</td>\n",
       "      <td>3</td>\n",
       "      <td>879138235</td>\n",
       "      <td>Kolya (1996)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>306</td>\n",
       "      <td>242</td>\n",
       "      <td>5</td>\n",
       "      <td>876503793</td>\n",
       "      <td>Kolya (1996)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user  movie  rating  timestamp         title\n",
       "0   196    242       3  881250949  Kolya (1996)\n",
       "1    63    242       3  875747190  Kolya (1996)\n",
       "2   226    242       5  883888671  Kolya (1996)\n",
       "3   154    242       3  879138235  Kolya (1996)\n",
       "4   306    242       5  876503793  Kolya (1996)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merge this with our ratings table to get the user ratings by title:\n",
    "\n",
    "ratings = ratings.merge(movies)\n",
    "ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "D5TkZw2CsQ-j",
    "outputId": "f19b3017-9190-4409-c1fd-f36a30120f11"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>title</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>640</td>\n",
       "      <td>Hard Target (1993)</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>795</td>\n",
       "      <td>Everyone Says I Love You (1996)</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>344</td>\n",
       "      <td>Searching for Bobby Fischer (1993)</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>201</td>\n",
       "      <td>Escape from New York (1981)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>207</td>\n",
       "      <td>People vs. Larry Flynt, The (1996)</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>435</td>\n",
       "      <td>Prophecy II, The (1998)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>645</td>\n",
       "      <td>Apollo 13 (1995)</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>429</td>\n",
       "      <td>Little Lord Fauntleroy (1936)</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>308</td>\n",
       "      <td>Last of the Mohicans, The (1992)</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>406</td>\n",
       "      <td>Dial M for Murder (1954)</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# build a DataLoaders object from this table\n",
    "# need to change the value of item_name to use titles instead of IDs:\n",
    "\n",
    "dls = CollabDataLoaders.from_df(ratings, item_name='title', bs=64)\n",
    "dls.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "U0cg3KmCtS9X"
   },
   "outputs": [],
   "source": [
    "# can't use the crosstab representation directly\n",
    "# so represent our movie and user latent factor tables as simple matrices:\n",
    "\n",
    "n_users = len(dls.classes['user'])\n",
    "n_movies = len(dls.classes['title'])\n",
    "n_factors = 5\n",
    "\n",
    "user_factors = torch.randn(n_users, n_factors)\n",
    "movie_factors = torch.randn(n_movies, n_factors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F1oSsUYq3koe"
   },
   "source": [
    "To calculate the combination:\n",
    "- look up the index of movie and user in respective latent factor matrix.\n",
    "- dot product between the two latent factor vectors.\n",
    "\n",
    "We can represent look up in an index as a matrix product: replace our indices with one-hot-encoded vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "py6u8TmcuR8V",
    "outputId": "37035371-a710-4233-9137-b80131718253"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.2728, -2.2764, -0.1875,  1.3184, -1.8212])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if we multiply a vector by a one-hot-encoded vector representing the index 3:\n",
    "\n",
    "one_hot_3 = one_hot(3, n_users).float()\n",
    "user_factors.t() @ one_hot_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DpzjKKNX43Rs",
    "outputId": "949ecb61-73b9-4605-e228-75489002f768"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.2728, -2.2764, -0.1875,  1.3184, -1.8212])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# it gives us the same vector as the one at index in the matrix:\n",
    "\n",
    "user_factors[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rkJG2c1PMOZQ"
   },
   "source": [
    "Pytorch include a special layer that indexes into a vector using an integer, but has its derivative calculated in such a way that it is identical to what it would have been if it had done a matrix multiplication with a one-hot-encoded vector. This is called an embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NtUOTIsDCP_c"
   },
   "source": [
    "In computer vision, easy to get pixel values in three numbers of RGB format but we don't have say way to characterize a user or a movie. There are probably relations with genres. \n",
    "\n",
    "We don't determine the numbers of characterize instead we will let our model learn them. By analysing existing relations between users and movies.\n",
    "\n",
    "Embedding: \n",
    "- attribute to each of users and each of movies a random vector of a certain length\n",
    "- make those learnable parameters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mKPur8QuVcQY"
   },
   "source": [
    "## Collaborative Filtering from Scratch\n",
    "Creating a new PyTorch module requires inheriting(add additional behaviour to an existing class) from __Module__.\n",
    "\n",
    "To create a new PyTorch module is that when your module is called, PyTorch will call a method in your class called __forward__, and will pass any parameters that are included in the call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "7olteiSl7Ebx"
   },
   "outputs": [],
   "source": [
    "# class defining dot product model:\n",
    "\n",
    "class DotProduct(Module):\n",
    "  def __init__(self, n_users, n_movies, n_factors):\n",
    "    self.user_factors = Embedding(n_users, n_factors)\n",
    "    self.movie_factors = Embedding(n_movies, n_factors)\n",
    "\n",
    "  def forward(self, x):\n",
    "    users = self.user_factors(x[:,0])\n",
    "    movies = self.movie_factors(x[:,1])\n",
    "    return (users*movies).sum(dim=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xlAaSGZrwj_9"
   },
   "source": [
    "Input of the model is a tensor of shape batch_size x 2:\n",
    "- first column(x[:,0]) : user IDs\n",
    "- second column(x[:,1]) : movie IDs\n",
    "\n",
    "We use the embedding layers to represent our matrices of user and movie latent factors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hr_k0KBrwjmg",
    "outputId": "dff40485-16d7-40ae-c775-0abe5639e346"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 2])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = dls.one_batch()\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "6vUzIdcEx5VO"
   },
   "outputs": [],
   "source": [
    "# create a Learner to optimize our model:\n",
    "\n",
    "model = DotProduct(n_users, n_movies, 50)\n",
    "learn = Learner(dls, model, loss_func=MSELossFlat())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "Y3T--uT0yi3j",
    "outputId": "dfe9a00d-9c29-4add-adab-26a1ab5847d7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.338174</td>\n",
       "      <td>1.233714</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.110292</td>\n",
       "      <td>1.056850</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.949190</td>\n",
       "      <td>0.948856</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.858994</td>\n",
       "      <td>0.867887</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.791723</td>\n",
       "      <td>0.852385</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# fit our model:\n",
    "\n",
    "learn.fit_one_cycle(5, 5e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Us2wY0e81pFU"
   },
   "source": [
    "To make the model bit better is to force those predictions to be between 0 and 5, by using sigmoid_range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "RYSEbwMsypeS",
    "outputId": "f9c3f5a3-2725-4963-8705-689a96e0c26c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.014979</td>\n",
       "      <td>0.968196</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.867497</td>\n",
       "      <td>0.873684</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.672310</td>\n",
       "      <td>0.842700</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.480007</td>\n",
       "      <td>0.852948</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.363641</td>\n",
       "      <td>0.857822</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# it's better to have the range go a little bit over 5, so we use (0, 5.5):\n",
    "\n",
    "class DotProduct(Module):\n",
    "    def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)):\n",
    "        self.user_factors = Embedding(n_users, n_factors)\n",
    "        self.movie_factors = Embedding(n_movies, n_factors)\n",
    "        self.y_range = y_range\n",
    "    def forward(self, x):\n",
    "        users = self.user_factors(x[:,0])\n",
    "        movies = self.movie_factors(x[:,1])\n",
    "        return sigmoid_range((users * movies).sum(dim=1), *self.y_range)\n",
    "\n",
    "model = DotProduct(n_users, n_movies, 50)\n",
    "learn = Learner(dls, model, loss_func=MSELossFlat())\n",
    "learn.fit_one_cycle(5, 5e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2h2wqHAEZ4gd"
   },
   "source": [
    "One obvious missing piece is that we have only weights do not have biases, causes:\n",
    "- some users are more +ve/-ve in their recommendations than others\n",
    "- some movies are plain better/worse than others\n",
    "\n",
    "We have a single number for each user that we can add to our scores, and ditto for each movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "_LTx7IAA3Q9n"
   },
   "outputs": [],
   "source": [
    "# adjust our model architecture:\n",
    "\n",
    "class DotProductBias(Module):\n",
    "    def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)):\n",
    "        self.user_factors = Embedding(n_users, n_factors)\n",
    "        self.user_bias = Embedding(n_users, 1)\n",
    "        self.movie_factors = Embedding(n_movies, n_factors)\n",
    "        self.movie_bias = Embedding(n_movies, 1)\n",
    "        self.y_range = y_range\n",
    "\n",
    "    def forward(self, x):\n",
    "        users = self.user_factors(x[:,0])\n",
    "        movies = self.movie_factors(x[:,1])\n",
    "        res = (users * movies).sum(dim=1, keepdim=True)\n",
    "        res += self.user_bias(x[:,0]) + self.movie_bias(x[:,1])\n",
    "        return sigmoid_range(res, *self.y_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "JGCBQfTdb5Hn",
    "outputId": "d33ea4d4-3c03-4f48-d64f-a0f424401982"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.951718</td>\n",
       "      <td>0.926971</td>\n",
       "      <td>00:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.844999</td>\n",
       "      <td>0.828742</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.621171</td>\n",
       "      <td>0.835285</td>\n",
       "      <td>00:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.421220</td>\n",
       "      <td>0.856117</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.308912</td>\n",
       "      <td>0.863082</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# try training this and see how it goes:\n",
    "\n",
    "model = DotProductBias(n_users, n_movies, 50)\n",
    "learn = Learner(dls, model, loss_func=MSELossFlat())\n",
    "learn.fit_one_cycle(5, 5e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TQjwgrbLfHKe"
   },
   "source": [
    "Ends up being worse, validation loss stopped improving in the middle and started to get worse: overfitting.\n",
    "\n",
    "There is no way data augmentation, so use another regularization technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MbPsejHNfnXw"
   },
   "source": [
    "### Weight Decay(*L2 regularization*)\n",
    "adding to loss fn the sum of all the weights squared: on computing the gradients, it will add a contribution to them that will encourage the weights to be as small as possible.\n",
    "\n",
    "The larger the coefficients are, the sharper canyons we will have in the loss fn: prevent overfitting\n",
    "\n",
    "- model learn high params cause it to fit all the data points in the training set\n",
    "- overcomplex func that has very sharp changes lead to overfitting\n",
    "\n",
    "Weight decay is a paramerter that controls that sum of squares we add to our loss(assuming parameters is a tensor of all parameters):\n",
    "```\n",
    "loss_with_wd = loss + wd * (parameters ** 2).sum()\n",
    "```\n",
    "adding that big sum to our loss is exactly the same as doing this:\n",
    "```\n",
    "parameters.grad += wd * 2 * parameters\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "7SWdatv9cRvh",
    "outputId": "28dace40-732f-4c78-9e4f-acd0bbc9bdb1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.943534</td>\n",
       "      <td>0.911988</td>\n",
       "      <td>00:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.865086</td>\n",
       "      <td>0.850096</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.746888</td>\n",
       "      <td>0.809093</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.598758</td>\n",
       "      <td>0.793388</td>\n",
       "      <td>00:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.498961</td>\n",
       "      <td>0.793942</td>\n",
       "      <td>00:11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# use weight decay in fastai, pass wd in fit_one_cycle:\n",
    "\n",
    "model = DotProductBias(n_users, n_movies, 50)\n",
    "learn = Learner(dls, model, loss_func=MSELossFlat())\n",
    "learn.fit_one_cycle(5, 5e-3, wd=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k51vSwFAtYkb"
   },
   "source": [
    "### Creating Our Own Embedding Module\n",
    "Re-create DotProductBias without using Embedding class. Need randomly initialized weight matrix for each of the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3bnUBSlIs7uB",
    "outputId": "f4bfa1a8-0525-406f-ba76-f75289862cbe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#0) []"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# adding a tensor as an attribute to a Module, will not included in parameters:\n",
    "\n",
    "class T(Module):\n",
    "    def __init__(self): self.a = torch.ones(3)\n",
    "\n",
    "L(T().parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "da2gusA3y_J7"
   },
   "source": [
    "To tell Module that we want to treat a tensor as a parameter, we have to wrap it in the nn.Parameter class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D7ZHf8SpyzRw",
    "outputId": "16671e38-2152-49ab-f5ff-7bbee90c3bc8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#1) [Parameter containing:\n",
       "tensor([1., 1., 1.], requires_grad=True)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# It's used only as a \"marker\" to showw what to includein parameters:\n",
    "\n",
    "class T(Module):\n",
    "    def __init__(self): self.a = nn.Parameter(torch.ones(3))\n",
    "\n",
    "L(T().parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KRa22cMyzcfe",
    "outputId": "4ee89628-1470-4c7c-8524-45913c0b45ce"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#1) [Parameter containing:\n",
       "tensor([[0.4440],\n",
       "        [0.3791],\n",
       "        [0.2014]], requires_grad=True)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all PyTorch modules use nn.Parameter for any trainable parameters:\n",
    "\n",
    "class T(Module):\n",
    "    def __init__(self): self.a = nn.Linear(1, 3, bias=False)\n",
    "\n",
    "t = T()\n",
    "L(t.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dgn-wovQz7J5",
    "outputId": "56d46bb0-f98b-4f84-be01-f6cc82505312"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.nn.parameter.Parameter"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(t.a.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "9G2C6ol-0AV9"
   },
   "outputs": [],
   "source": [
    "# create a tensor as a parameter, with random initialization:\n",
    "\n",
    "def create_params(size):\n",
    "    return nn.Parameter(torch.zeros(*size).normal_(0, 0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "TafM_BHF0UCk"
   },
   "outputs": [],
   "source": [
    "# use this to create DotProductBias, without Embedding:\n",
    "\n",
    "class DotProductBias(Module):\n",
    "    def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)):\n",
    "        self.user_factors = create_params([n_users, n_factors])\n",
    "        self.user_bias = create_params([n_users])\n",
    "        self.movie_factors = create_params([n_movies, n_factors])\n",
    "        self.movie_bias = create_params([n_movies])\n",
    "        self.y_range = y_range\n",
    "\n",
    "    def forward(self, x):\n",
    "        users = self.user_factors[x[:,0]]\n",
    "        movies = self.movie_factors[x[:,1]]\n",
    "        res = (users*movies).sum(dim=1)\n",
    "        res += self.user_bias[x[:,0]] + self.movie_bias[x[:,1]]\n",
    "        return sigmoid_range(res, *self.y_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "OSH3igGy2TXF",
    "outputId": "8fd68e5f-e32f-4b13-dd3c-d11c0a78a314"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.942291</td>\n",
       "      <td>0.912609</td>\n",
       "      <td>00:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.886895</td>\n",
       "      <td>0.851541</td>\n",
       "      <td>00:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.717332</td>\n",
       "      <td>0.805790</td>\n",
       "      <td>00:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.593627</td>\n",
       "      <td>0.792649</td>\n",
       "      <td>00:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.483319</td>\n",
       "      <td>0.792937</td>\n",
       "      <td>00:11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# train it again to check we get around the same results as before:\n",
    "\n",
    "model = DotProductBias(n_users, n_movies, 50)\n",
    "learn = Learner(dls, model, loss_func=MSELossFlat())\n",
    "learn.fit_one_cycle(5, 5e-3, wd=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KYzTITTP2-4L"
   },
   "source": [
    "## Interpreting Embeddings and Biases\n",
    "model ca provide us with movie recommendations for our users. The easiest to interpret are the biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6xyWl8PR24gf",
    "outputId": "a3cc07fb-e9f0-4068-f1db-a611b65c873c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Children of the Corn: The Gathering (1996)',\n",
       " 'Mortal Kombat: Annihilation (1997)',\n",
       " 'Lawnmower Man 2: Beyond Cyberspace (1996)',\n",
       " 'Barb Wire (1996)',\n",
       " 'Robocop 3 (1993)']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the movies with the lowest values in the bias vector:\n",
    "\n",
    "movie_bias = learn.model.movie_bias.squeeze()\n",
    "idxs = movie_bias.argsort()[:5]\n",
    "[dls.classes['title'][i] for i in idxs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FJdmTB9i5u5Y"
   },
   "source": [
    "It's saying is that for each of these movies, even when a user is very well matched to its latent factors, they still generally don't like it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lmCDCuds4-2n",
    "outputId": "dff0835a-3d44-42ef-8fa2-43f79c82b76c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Titanic (1997)',\n",
       " 'Shawshank Redemption, The (1994)',\n",
       " 'L.A. Confidential (1997)',\n",
       " \"Schindler's List (1993)\",\n",
       " 'Silence of the Lambs, The (1991)']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# by the same token, here are the movies with the hightest bias:\n",
    "\n",
    "idxs = movie_bias.argsort(descending=True)[:5]\n",
    "[dls.classes['title'][i] for i in idxs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r2pxJ_fM7Ck8"
   },
   "source": [
    "It's not easy to directly interpret the embedding matrices. There're too many factors for a human to look at. But there is a technique that can pull out the most important underlying directions in such a matrix, called principal component analysis (PCA)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CuPzZn0i-WC4"
   },
   "source": [
    "### Using fastai.collab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "7q4SLC9O6Ydl",
    "outputId": "c77f43d8-9ada-4d4b-893a-d144ab08f9b7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.951376</td>\n",
       "      <td>0.910982</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.869922</td>\n",
       "      <td>0.844780</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.734434</td>\n",
       "      <td>0.806652</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.597746</td>\n",
       "      <td>0.794313</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.476012</td>\n",
       "      <td>0.795036</td>\n",
       "      <td>00:11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create and train a collaborative filtering model:\n",
    "\n",
    "learn = collab_learner(dls, n_factors=50, y_range=(0, 5.5))\n",
    "learn.fit_one_cycle(5, 5e-3, wd=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cPjWUyFo-6mE",
    "outputId": "579d684d-c7fc-4d5a-fee1-861d1310b681"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EmbeddingDotBias(\n",
       "  (u_weight): Embedding(944, 50)\n",
       "  (i_weight): Embedding(1665, 50)\n",
       "  (u_bias): Embedding(944, 1)\n",
       "  (i_bias): Embedding(1665, 1)\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the names of the layers can be seen by printing the model:\n",
    "\n",
    "learn.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mZY8LPrb_C9m",
    "outputId": "11f9d275-5cca-425a-ceab-515739e698d7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Titanic (1997)',\n",
       " 'Shawshank Redemption, The (1994)',\n",
       " \"Schindler's List (1993)\",\n",
       " 'L.A. Confidential (1997)',\n",
       " 'Star Wars (1977)']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can use these to replicate any of the analyses did before:\n",
    "\n",
    "movie_bias = learn.model.i_bias.weight.squeeze()\n",
    "idxs = movie_bias.argsort(descending=True)[:5]\n",
    "[dls.classes['title'][i] for i in idxs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mqXe9ntn_4WE"
   },
   "source": [
    "### Embedding Distance\n",
    "Movie similarity can be defined by the similarity of users who like those movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "yBUTHEE2_mbU",
    "outputId": "95dec0f8-01e9-4df2-c913-94c96d471f8b"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"One Flew Over the Cuckoo's Nest (1975)\""
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use this to find the most similar movie to Silence of the Lambs:\n",
    "\n",
    "movie_factors = learn.model.i_weight.weight\n",
    "idx = dls.classes['title'].o2i['Silence of the Lambs, The (1991)']\n",
    "distances = nn.CosineSimilarity(dim=1)(movie_factors, movie_factors[idx][None])\n",
    "idx = distances.argsort(descending=True)[1]\n",
    "dls.classes['title'][idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N9oWrHMYBrJq"
   },
   "source": [
    "## Bootstrapping a Collaborative Filtering Model\n",
    "The most extreme version of this problem is having no users, and therefore no history to learn from, then what product to recommend to very first user.\n",
    "\n",
    "Our dot product model works quite well, and it is the basis of many successful real-world recommendation systems. This approach to collaborative filtering is known as probabilistic matrix factorization(PMF)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7tdg3l-HEMBW"
   },
   "source": [
    "## Deep Learning for Collaborative Filtering\n",
    "To turn our architecture into a deep learning model\n",
    "- take the results of the embedding lookup and concatenate those activations together.\n",
    "\n",
    "Since we concatenating the embedding matrices, rather than taking their dot product, the two embedding matricess can have different sizes.\n",
    "\n",
    "fastai has a function get_emb_sz: returns recommended sizes for embedding matrices for your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bPTEh6pMBXns",
    "outputId": "5a9fb296-0901-46d3-a72a-d3118e55aac9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(944, 74), (1665, 102)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# based on a heuristic that fast.ai has found tends to work well in practice:\n",
    "\n",
    "embs = get_emb_sz(dls)\n",
    "embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "zK1PnBExGVq4"
   },
   "outputs": [],
   "source": [
    "# implement this class:\n",
    "\n",
    "class CollabNN(Module):\n",
    "    def __init__(self, user_sz, item_sz, y_range=(0, 5.5), n_act=100):\n",
    "        self.user_factors = Embedding(*user_sz)\n",
    "        self.item_factors = Embedding(*item_sz)\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(user_sz[1]+item_sz[1], n_act),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_act, 1)\n",
    "        )\n",
    "        self.y_range = y_range\n",
    "\n",
    "    def forward(self, x):\n",
    "        embs = self.user_factors(x[:,0]), self.item_factors(x[:,1])\n",
    "        x = self.layers(torch.cat(embs, dim=1))\n",
    "        return sigmoid_range(x, *self.y_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "4ldZ4aBbHTN5"
   },
   "outputs": [],
   "source": [
    "# and use it to create a model:\n",
    "\n",
    "model = CollabNN(*embs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PiIaQvKSHm5o"
   },
   "source": [
    "- CollabNN creates our Embedding layers as before, except that we now use embs sizes(self.layers is identical to the mini-neural net).\n",
    "- In forward,\n",
    "    - we apply embeddings\n",
    "    - concatenate the results\n",
    "    - pass this through the mini-neural net\n",
    "    - apply sigmoid_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "9seB7a-CHZpi",
    "outputId": "817ff86d-6a48-48a6-a10c-6230cc3d9b87"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.949596</td>\n",
       "      <td>0.923606</td>\n",
       "      <td>00:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.919100</td>\n",
       "      <td>0.874231</td>\n",
       "      <td>00:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.847160</td>\n",
       "      <td>0.850019</td>\n",
       "      <td>00:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.837006</td>\n",
       "      <td>0.843700</td>\n",
       "      <td>00:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.789033</td>\n",
       "      <td>0.843859</td>\n",
       "      <td>00:11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# see if it trains:\n",
    "learn = Learner(dls, model, loss_func=MSELossFlat())\n",
    "learn.fit_one_cycle(5, 5e-3, wd=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OoilyCTpJM2A"
   },
   "source": [
    "fastai provides this model in fastai.collab if pass use_nn=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "hIfht-abJDcq",
    "outputId": "1df10e90-e869-49f0-fee1-6b0d520b646b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.976780</td>\n",
       "      <td>0.935480</td>\n",
       "      <td>00:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.920364</td>\n",
       "      <td>0.884013</td>\n",
       "      <td>00:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.884336</td>\n",
       "      <td>0.859847</td>\n",
       "      <td>00:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.834246</td>\n",
       "      <td>0.831388</td>\n",
       "      <td>00:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.798400</td>\n",
       "      <td>0.831519</td>\n",
       "      <td>00:13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# creating two hidden layers, of size 100 and 50:\n",
    "\n",
    "learn = collab_learner(dls, use_nn=True, y_range=(0, 5.5), layers=[100, 50])\n",
    "learn.fit_one_cycle(5, 5e-3, wd=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "sV2r2nhdJul7"
   },
   "outputs": [],
   "source": [
    "# learn.model is an object of type EmbeddingNN:\n",
    "\n",
    "@delegates(TabularModel)\n",
    "class EmbeddingNN(TabularModel):\n",
    "    def __init__(self, emb_szs, layers, **kwargs):\n",
    "        super().__init__(emb_szs, layers=layers, n_cont=0, out_sz=1, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t-AujyQPKOUQ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "008CollaborativeFilteringDeepDive.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
